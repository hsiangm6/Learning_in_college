{"cells":[{"cell_type":"markdown","metadata":{"id":"P6Z1Snuk7rIK"},"source":["# MIS 583 Assignment 6: Text Sentiment Classification with Prompt Learning"]},{"cell_type":"markdown","metadata":{"id":"oSwr9MgZogRZ"},"source":["Before we start, please put your name and SID in following format: <br>\n",": LASTNAME Firstname, ?00000000   //   e.g.) 李晨愷, M114020035"]},{"cell_type":"markdown","metadata":{"id":"6DzsjuDhlz_k"},"source":["**Your Answer:**   \n","Hi I'm 危湘妤, B104020011"]},{"cell_type":"markdown","metadata":{"id":"5d-Zzebq7rIM"},"source":["## Overview"]},{"cell_type":"markdown","metadata":{"id":"Kc9gd_Wk7rIN"},"source":["**Sentiment Classification** is an automated process of identifying opinions in text and labeling them as positive or negative based on the emotions customers express within them.\n","\n","In Task 1, you need to fine-tune a pre-trained language model (e.g., BERT) to predict the sentiment of given tweets.\n","\n","In Task 2, we employ prompts to enable the model to perform sentiment analysis through in-context learning, eliminating the need for additional training.\n","\n","In Task 3, you will use the method called LM-BFF to utilize the model in generating the optimal template and verbalizer autonomously.\n"]},{"cell_type":"markdown","metadata":{"id":"zH2hlSWxR506"},"source":["# Notice\n","**You are not allow to use the model like GPT family or pre-trained weight using SST-2 and twitter dataset!!!!!!!!!!!!!!!!!**\n","\n","You can use BERT and RoBERTa encoder model"]},{"cell_type":"markdown","metadata":{"id":"giUId1Naqacs","tags":[]},"source":["##  Versions of used packages\n","\n","We will check PyTorch version to make sure everything work properly.  \n","We use `python==3.7.14`, `torch==1.12.1+cu113` and `torchvision==0.13.1+cu113`.  \n","This is the default version in Google Colab."]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10265,"status":"ok","timestamp":1703062742868,"user":{"displayName":"m6","userId":"16445725425625979130"},"user_tz":-480},"id":"Vuw-gNvjqcYe","outputId":"5fd9fc7f-ff7b-485f-b09a-d0bb9685e39d"},"outputs":[{"name":"stdout","output_type":"stream","text":["python 3.10.12 (main, Nov 20 2023, 15:14:05) [GCC 11.4.0]\n","torch 2.1.0+cu121\n","torchvision 0.16.0+cu121\n"]}],"source":["import sys\n","import torch\n","import torchvision\n","print('python', sys.version.split('\\n')[0])\n","print('torch', torch.__version__)\n","print('torchvision', torchvision.__version__)"]},{"cell_type":"markdown","metadata":{"id":"DEOiQHVKR508"},"source":["# Task 1: Text Sentiment Classification (40 points)\n","\n","In this task, you need to fine-tune a pre-trained language model (e.g., BERT or RoBERTa encoder) to predict the sentiment of given tweets.\n"]},{"cell_type":"markdown","metadata":{"id":"0a4s_a5D7rIR"},"source":["## Loading Model and Data"]},{"cell_type":"markdown","metadata":{"id":"GPUkTbnL7rIR"},"source":["First, let's talk about the model. The Hugging Face team has created an amazing framework called \"transformers\" for NLP tasks. It includes many state-of-the-art machine learning models for PyTorch, TensorFlow, and JAX.\n","\n","To start with this package, follow [this link to installation and a basic tutorial](https://pytorch.org/hub/huggingface_pytorch-transformers/)."]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":40665,"status":"ok","timestamp":1703062936783,"user":{"displayName":"m6","userId":"16445725425625979130"},"user_tz":-480},"id":"rK0ouXa09pDU","outputId":"456b3ba6-4d06-4ffa-898e-5e2ebab780d8"},"outputs":[{"name":"stdout","output_type":"stream","text":["happy installation\n","pip 23.1.2 from /usr/local/lib/python3.10/dist-packages/pip (python 3.10)\n","Requirement already satisfied: grpcio in /usr/local/lib/python3.10/dist-packages (1.60.0)\n","Requirement already satisfied: google-auth in /usr/local/lib/python3.10/dist-packages (2.17.3)\n","Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth) (5.3.2)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth) (0.3.0)\n","Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from google-auth) (1.16.0)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth) (4.9)\n","Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth) (0.5.1)\n","Collecting protobuf==3.9.2\n","  Downloading protobuf-3.9.2-py2.py3-none-any.whl (431 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m431.5/431.5 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from protobuf==3.9.2) (67.7.2)\n","Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.10/dist-packages (from protobuf==3.9.2) (1.16.0)\n","Installing collected packages: protobuf\n","  Attempting uninstall: protobuf\n","    Found existing installation: protobuf 3.20.3\n","    Uninstalling protobuf-3.20.3:\n","      Successfully uninstalled protobuf-3.20.3\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","google-ai-generativelanguage 0.4.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.9.2 which is incompatible.\n","google-api-core 2.11.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0.dev0,>=3.19.5, but you have protobuf 3.9.2 which is incompatible.\n","google-cloud-aiplatform 1.38.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.9.2 which is incompatible.\n","google-cloud-bigquery 3.12.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.9.2 which is incompatible.\n","google-cloud-bigquery-connection 1.12.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.9.2 which is incompatible.\n","google-cloud-bigquery-storage 2.24.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.9.2 which is incompatible.\n","google-cloud-datastore 2.15.2 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.9.2 which is incompatible.\n","google-cloud-firestore 2.11.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.9.2 which is incompatible.\n","google-cloud-functions 1.13.3 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.9.2 which is incompatible.\n","google-cloud-iam 2.13.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.9.2 which is incompatible.\n","google-cloud-language 2.9.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.9.2 which is incompatible.\n","google-cloud-resource-manager 1.11.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.9.2 which is incompatible.\n","google-cloud-translate 3.11.3 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.9.2 which is incompatible.\n","googleapis-common-protos 1.62.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0.dev0,>=3.19.5, but you have protobuf 3.9.2 which is incompatible.\n","grpc-google-iam-v1 0.13.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.9.2 which is incompatible.\n","grpcio-status 1.48.2 requires protobuf>=3.12.0, but you have protobuf 3.9.2 which is incompatible.\n","proto-plus 1.23.0 requires protobuf<5.0.0dev,>=3.19.0, but you have protobuf 3.9.2 which is incompatible.\n","tensorboard 2.15.1 requires protobuf<4.24,>=3.19.6, but you have protobuf 3.9.2 which is incompatible.\n","tensorflow 2.15.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3, but you have protobuf 3.9.2 which is incompatible.\n","tensorflow-datasets 4.9.3 requires protobuf>=3.20, but you have protobuf 3.9.2 which is incompatible.\n","tensorflow-hub 0.15.0 requires protobuf>=3.19.6, but you have protobuf 3.9.2 which is incompatible.\n","tensorflow-metadata 1.14.0 requires protobuf<4.21,>=3.20.3, but you have protobuf 3.9.2 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed protobuf-3.9.2\n"]},{"data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["google"]}}},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Collecting pyprind\n","  Downloading PyPrind-2.11.3-py2.py3-none-any.whl (8.4 kB)\n","Installing collected packages: pyprind\n","Successfully installed pyprind-2.11.3\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.1)\n","Collecting boto3\n","  Downloading boto3-1.34.4-py3-none-any.whl (139 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.3/139.3 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.31.0)\n","Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (2023.6.3)\n","Collecting sentencepiece\n","  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting sacremoses\n","  Downloading sacremoses-0.1.1-py3-none-any.whl (897 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m897.5/897.5 kB\u001b[0m \u001b[31m50.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting botocore<1.35.0,>=1.34.4 (from boto3)\n","  Downloading botocore-1.34.4-py3-none-any.whl (11.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.8/11.8 MB\u001b[0m \u001b[31m43.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting jmespath<2.0.0,>=0.7.1 (from boto3)\n","  Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n","Collecting s3transfer<0.10.0,>=0.9.0 (from boto3)\n","  Downloading s3transfer-0.9.0-py3-none-any.whl (82 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.0/82.0 kB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2023.11.17)\n","Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from sacremoses) (8.1.7)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from sacremoses) (1.3.2)\n","Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.10/dist-packages (from botocore<1.35.0,>=1.34.4->boto3) (2.8.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.35.0,>=1.34.4->boto3) (1.16.0)\n","Installing collected packages: sentencepiece, sacremoses, jmespath, botocore, s3transfer, boto3\n","Successfully installed boto3-1.34.4 botocore-1.34.4 jmespath-1.0.1 s3transfer-0.9.0 sacremoses-0.1.1 sentencepiece-0.1.99\n"]}],"source":["# you might need some additional installations there\n","!echo happy installation\n","!pip -V\n","!pip install grpcio\n","!pip install google-auth\n","!pip install protobuf==3.9.2\n","!pip install pyprind\n","!pip install tqdm boto3 requests regex sentencepiece sacremoses"]},{"cell_type":"code","execution_count":39,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2311,"status":"ok","timestamp":1703006682803,"user":{"displayName":"m6","userId":"16445725425625979130"},"user_tz":-480},"id":"dmGCAevi7rIS","outputId":"8779b344-92ef-4232-835a-ef424511fee3"},"outputs":[{"name":"stderr","output_type":"stream","text":["Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}],"source":["from torch import nn\n","\n","#########################################################################\n","#            Loading tokenizer and model from transformer               #\n","#########################################################################\n","# Bert\n","# from transformers import BertTokenizer, BertForSequenceClassification\n","\n","# bert_type = 'bert-base-uncased'\n","\n","# tokenizer = BertTokenizer.from_pretrained(bert_type)\n","# model = BertForSequenceClassification.from_pretrained(bert_type)\n","\n","# Roberta\n","from transformers import RobertaTokenizer, RobertaForSequenceClassification\n","\n","bert_type = 'roberta-base'\n","\n","tokenizer = RobertaTokenizer.from_pretrained(bert_type)\n","\n","model = RobertaForSequenceClassification.from_pretrained(bert_type)\n","\n","# finetune from the output from bert to your task\n","model.classifier.out_proj = nn.Linear(in_features=768, out_features=3, bias=True)\n","# model.classifier = nn.Linear(768, 3, bias=True)\n","# dropout_rate = 0.1  # 设置你想要的dropout率\n","# model.classifier = nn.Sequential(\n","#     nn.Dropout(dropout_rate),\n","#     nn.Linear(768, 3, bias=True)\n","# )\n","#########################################################################\n","#                          End of your code                             #\n","#########################################################################"]},{"cell_type":"markdown","metadata":{"id":"IiMThsYeDa2O"},"source":["## How to Get Data\n","\n","Please open the file `twitter_sentiment.zip`, creat shortcut to your Google Drive.\n","\n","1. open [LINK of Google Drive](https://drive.google.com/file/d/19Ty2lVAm55VL5QIM-MMQhhOzWXeMtxeV/view?usp=sharing)\n","2. Click \"Add shortcut to Drive\" in the top-right corner.\n","3. Select the location where you want to place the shortcut.\n","4. Click Add shortcut.\n","\n","After above procedures, we have a shortcut of zip file of dataset.  \n","We can access this in colab after granting the permission of Google Drive.\n","\n","---\n","\n","請先到共用雲端硬碟將檔案 `twitter_sentiment.zip`，建立捷徑到自己的雲端硬碟中。\n","\n","> 操作步驟\n","1. 點開雲端[連結](https://drive.google.com/file/d/19Ty2lVAm55VL5QIM-MMQhhOzWXeMtxeV/view?usp=sharing)\n","2. 點選右上角「新增雲端硬碟捷徑」\n","3. 點選「我的雲端硬碟」\n","4. 點選「新增捷徑」\n","\n","完成以上流程會在你的雲端硬碟中建立一個檔案的捷徑，接著我們在colab中取得權限即可使用。"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":23943,"status":"ok","timestamp":1703078516365,"user":{"displayName":"m6","userId":"16445725425625979130"},"user_tz":-480},"id":"lZnFgi5i_2oA","outputId":"440b4ee7-3e06-4460-b88d-dea357c73517"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"markdown","metadata":{"id":"cqO8DiB6VRQZ"},"source":["## Unzip Data\n","\n","解壓縮 `twitter_sentiment.zip` 後可以發現裡面有三個csv檔。\n","\n","- `train.csv`, `test.csv` and `val.csv`\n","\n","Training set 有 **10248** 筆資料.  \n","Validation set 有 **1317** 筆資料.  \n","Testing set 有 **3075** 筆資料.  \n","\n","注意: 若有另外設定存放在雲端硬碟中的路徑，請記得本處路徑也須做更動。"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":3367,"status":"ok","timestamp":1702975217684,"user":{"displayName":"m6","userId":"16445725425625979130"},"user_tz":-480},"id":"OSlTMdxf8Zd7"},"outputs":[],"source":["# !unzip -qq ./drive/MyDrive/Deep_Learning/A6/twitter_sentiment.zip -d ./drive/MyDrive/Deep_Learning/A6"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1703062961167,"user":{"displayName":"m6","userId":"16445725425625979130"},"user_tz":-480},"id":"ODDmH7ebOO1i","outputId":"3386d8d5-75f9-4769-d966-5c1e143a7ebe"},"outputs":[{"name":"stdout","output_type":"stream","text":["新的當前工作目錄： /content/drive/MyDrive/Deep_Learning/A6\n"]}],"source":["import os\n","# 要切換的目錄\n","new_directory = '/content/drive/MyDrive/Deep_Learning/A6'\n","\n","# 更改當前工作目錄\n","os.chdir(new_directory)\n","\n","# 更新當前工作目錄\n","current_path = os.getcwd()\n","print(\"新的當前工作目錄：\", current_path)"]},{"cell_type":"markdown","metadata":{"id":"4LxDMQzHR50_"},"source":["# Loading the dataset"]},{"cell_type":"code","execution_count":8,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1703004057204,"user":{"displayName":"m6","userId":"16445725425625979130"},"user_tz":-480},"id":"wf5GXTme7rIT"},"outputs":[],"source":["# Utility function to extract text and label from csv file\n","def get_texts(f_name='./twitter_sentiment', mode='train'):\n","    text_list = []\n","    label_list = []\n","\n","    f_path = os.path.join(f_name, '{}.csv'.format(mode))\n","    with open(f_path) as f:\n","        reader = csv.DictReader(f)\n","        for line in reader:\n","            text_list.append(line['text'])\n","            if mode != 'test':\n","                label_list.append(int(line['sentiment_label']))\n","\n","    return text_list, label_list"]},{"cell_type":"code","execution_count":9,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1703004057204,"user":{"displayName":"m6","userId":"16445725425625979130"},"user_tz":-480},"id":"6fpY0ZrK7rIV"},"outputs":[],"source":["import os\n","import csv\n","import torch\n","from torch.utils.data import Dataset, DataLoader\n","class TwitterDataset(Dataset):\n","    def __init__(self, f_name='./twitter_sentiment', mode='train'):\n","        self.mode = mode\n","\n","        text_list, label_list = get_texts(f_name, mode)\n","        print('mode', mode, 'has', len(text_list), 'datas')\n","        text_list = tokenizer(text_list,\n","                             truncation=True, padding=True,\n","                             return_tensors='pt')\n","\n","        self.text_list = text_list['input_ids']\n","        self.mask_list = text_list['attention_mask']\n","\n","        self.label_list = label_list\n","\n","    def __getitem__(self, idx):\n","        text = self.text_list[idx]\n","        mask = self.mask_list[idx]\n","        if self.mode == 'test':\n","            return text, mask\n","        label = torch.tensor(self.label_list[idx])\n","        return text, mask, label\n","\n","    def __len__(self):\n","        return len(self.text_list)"]},{"cell_type":"markdown","metadata":{"id":"R9wp2TUTR50_"},"source":["## `DataLoader`\n","\n","`torch.utils.data.DataLoader` define how to sample from `dataset` and some other function like:\n","+ `shuffle` : set to `True` to have the data reshuffled at every epoch\n","+ `batch_size` : how many samples per batch to load\n","\n","See [torch.utils.data.DataLoader](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader) for more details"]},{"cell_type":"code","execution_count":44,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7232,"status":"ok","timestamp":1703006906571,"user":{"displayName":"m6","userId":"16445725425625979130"},"user_tz":-480},"id":"nCmM4FSw7rIW","outputId":"e7e27109-86f5-4721-f8ee-1970d9525ec9"},"outputs":[{"name":"stdout","output_type":"stream","text":["mode train has 10248 datas\n","mode val has 1317 datas\n","mode test has 3075 datas\n"]}],"source":["dataset_train = TwitterDataset(mode='train')\n","dataset_val = TwitterDataset(mode='val')\n","dataset_test = TwitterDataset(mode='test')\n","\n","batch_size = 32\n","train_data = DataLoader(dataset_train, batch_size=batch_size,\n","                       shuffle=True)\n","val_data = DataLoader(dataset_val, batch_size=batch_size // 2,\n","                       shuffle=False)\n","test_data = DataLoader(dataset_test, batch_size=batch_size // 2,\n","                       shuffle=False)"]},{"cell_type":"code","execution_count":45,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":850,"status":"ok","timestamp":1703006914708,"user":{"displayName":"m6","userId":"16445725425625979130"},"user_tz":-480},"id":"bqkvofHc7rIY","outputId":"1a64bcb8-4937-46e8-8dc7-2fe44765e99a"},"outputs":[{"name":"stdout","output_type":"stream","text":["token ['<s>', '@', 'united', 'ĠI', 'Ġhave', 'Ġnever', 'Ġbeen', 'Ġmislead', 'Ġby', 'Ġa', 'Ġcompany', 'Ġas', 'Ġmany', 'Ġtimes', 'Ġas', 'ĠI', 'Ġhave', 'Ġthis', 'Ġweek', 'Ġby', 'ĠUnited', 'ĠAirlines', '!', '</s>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n","token to s <s>@united I have never been mislead by a company as many times as I have this week by United Airlines!</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n"]}],"source":["t = tokenizer.convert_ids_to_tokens(dataset_train[0][0]) # converts a sequence of numeric IDs in the training dataset into their corresponding tokens using the specified tokenizer.\n","print('token', t)\n","print('token to s', tokenizer.convert_tokens_to_string(t)) # converts a sequence of tokens (t) back into the original text string using the specified tokenizer."]},{"cell_type":"markdown","metadata":{"id":"xqoS2ed-R51A"},"source":["# Define loss and optimizer"]},{"cell_type":"code","execution_count":47,"metadata":{"executionInfo":{"elapsed":388,"status":"ok","timestamp":1703006942672,"user":{"displayName":"m6","userId":"16445725425625979130"},"user_tz":-480},"id":"DxZrfCqW7rIY"},"outputs":[],"source":["device = torch.device('cuda')\n","\n","from torch import nn\n","from transformers import AdamW\n","\n","optimizer = AdamW(model.parameters(), lr=2e-5)\n","criterion = nn.CrossEntropyLoss()\n","\n","model = model.to(device)\n","criterion = criterion.to(device)"]},{"cell_type":"markdown","metadata":{"id":"UpwgE2Gd7rIZ"},"source":["# Utility Function"]},{"cell_type":"code","execution_count":48,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1703006944631,"user":{"displayName":"m6","userId":"16445725425625979130"},"user_tz":-480},"id":"zlaiAZAD7rIa"},"outputs":[],"source":["def accuracy(raw_preds, y):\n","    preds = raw_preds.argmax(dim=1)\n","    acc = (preds == y).sum()\n","    return acc"]},{"cell_type":"markdown","metadata":{"id":"RJlRH7RxR51A"},"source":["# Train function"]},{"cell_type":"code","execution_count":49,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1703006946156,"user":{"displayName":"m6","userId":"16445725425625979130"},"user_tz":-480},"id":"dmc_Gms97rIa"},"outputs":[],"source":["from tqdm import tqdm\n","import matplotlib.pyplot as plt\n","\n","train_loss_list = []\n","val_loss_list = []\n","\n","def train(model, data, optimizer, criterion):\n","    model.train()\n","\n","    epoch_loss = 0\n","    epoch_acc = 0\n","    total = 0\n","    for text, mask, label in tqdm(data, total=len(data)):\n","        text = text.to(device)\n","        mask = mask.to(device)\n","        label = label.to(device)\n","\n","        #########################################################################\n","        #                          Testing process                              #\n","        #########################################################################\n","        # 1. Clean the gradients of optimizer\n","        optimizer.zero_grad()\n","        # 2. Put correct variables into model\n","        outputs = model(text, attention_mask=mask)\n","        # 3. Get prediction\n","        preds = outputs.logits\n","        # 4. Evaluate by criterion and accuracy\n","        loss = criterion(preds, label)\n","        acc = accuracy(preds, label)\n","        #########################################################################\n","        #                          End of your code                             #\n","        #########################################################################\n","\n","        loss.backward()\n","        optimizer.step()\n","\n","        epoch_loss += loss.item()\n","        train_loss_list.append(loss.item())\n","        epoch_acc += acc.item()\n","        total += len(text)\n","    return epoch_loss / total, epoch_acc / total\n","\n","def test(model, data, criterion, log_loss=False):\n","    model.eval()\n","\n","    epoch_loss = 0\n","    epoch_acc = 0\n","    total = 0\n","    for text, mask, label in tqdm(data, total=len(data)):\n","        text = text.to(device)\n","        mask = mask.to(device)\n","        label = label.to(device)\n","\n","        #########################################################################\n","        #                          Training process                             #\n","        #########################################################################\n","        with torch.no_grad():  # 使用torch.no_grad()上下文管理器，以禁用梯度计算\n","          # 2. Get prediction\n","          outputs = model(text, attention_mask=mask)\n","          preds = outputs.logits\n","          # 3. Evaluate by criterion and accuracy\n","          loss = criterion(preds, label)\n","          acc = accuracy(preds, label)\n","\n","        #########################################################################\n","        #                          End of your code                             #\n","        #########################################################################\n","\n","        epoch_loss += loss.item()\n","        if log_loss:\n","            val_loss_list.append(loss.item())\n","        epoch_acc += acc.item()\n","        total += len(text)\n","    return epoch_loss / total, epoch_acc / total\n","\n","# class for monitoring train and test acc/loss\n","class Meter:\n","    def __init__(self):\n","        self.train_loss_list = []\n","        self.train_acc_list = []\n","        self.val_loss_list = []\n","        self.val_acc_list = []\n","\n","    def update(self, train_loss, train_acc, val_loss, val_acc):\n","        self.train_loss_list.append(train_loss)\n","        self.train_acc_list.append(train_acc)\n","        self.val_loss_list.append(val_loss)\n","        self.val_acc_list.append(val_acc)\n","\n","    def plot(self):\n","        x = range(len(self.train_loss_list))\n","        plt.plot(x, self.train_loss_list)\n","        plt.plot(x, self.val_loss_list, color='r')\n","        plt.legend(['train_loss', 'val_loss'])\n","        plt.show()\n","        plt.plot(x, self.train_acc_list)\n","        plt.plot(x, self.val_acc_list, color='r')\n","        plt.legend(['train_acc', 'val_acc'])\n","        plt.show()"]},{"cell_type":"markdown","metadata":{"id":"ExZyrKd57rIb"},"source":["# Start Training"]},{"cell_type":"code","execution_count":52,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2223940,"status":"ok","timestamp":1703010388213,"user":{"displayName":"m6","userId":"16445725425625979130"},"user_tz":-480},"id":"bVDe-fRe7rIc","outputId":"10bce7be-13e7-4011-ccca-c3fc4efaa02c"},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████| 321/321 [03:34<00:00,  1.49it/s]\n","100%|██████████| 83/83 [00:04<00:00, 17.56it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 1 train_loss: 0.003336603440056067 train_acc: 0.9642857142857143\n","Epoch 1 val_loss:  0.03245168613519551 val_acc : 0.8519362186788155\n","---------- e 1 save best model ----------\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 321/321 [03:34<00:00,  1.49it/s]\n","100%|██████████| 83/83 [00:04<00:00, 17.74it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 2 train_loss: 0.0025550798821027823 train_acc: 0.9737509758001561\n","Epoch 2 val_loss:  0.03970164918795443 val_acc : 0.8352315869400152\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 321/321 [03:34<00:00,  1.50it/s]\n","100%|██████████| 83/83 [00:04<00:00, 17.84it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 3 train_loss: 0.002184439279741748 train_acc: 0.9786299765807962\n","Epoch 3 val_loss:  0.04055907173703114 val_acc : 0.8473804100227791\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 321/321 [03:34<00:00,  1.50it/s]\n","100%|██████████| 83/83 [00:04<00:00, 17.77it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 4 train_loss: 0.001812735963916329 train_acc: 0.9824355971896955\n","Epoch 4 val_loss:  0.03956066874916974 val_acc : 0.8663629460895975\n","---------- e 4 save best model ----------\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 321/321 [03:34<00:00,  1.50it/s]\n","100%|██████████| 83/83 [00:04<00:00, 17.66it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 5 train_loss: 0.0014615521244398463 train_acc: 0.985655737704918\n","Epoch 5 val_loss:  0.05082717946504795 val_acc : 0.8420652999240699\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 321/321 [03:34<00:00,  1.50it/s]\n","100%|██████████| 83/83 [00:04<00:00, 17.69it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 6 train_loss: 0.0016256246638105699 train_acc: 0.9828259172521467\n","Epoch 6 val_loss:  0.04510284655318357 val_acc : 0.8413059984813971\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 321/321 [03:34<00:00,  1.49it/s]\n","100%|██████████| 83/83 [00:04<00:00, 17.56it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 7 train_loss: 0.0015674204491677743 train_acc: 0.9838017174082748\n","Epoch 7 val_loss:  0.04439527188926644 val_acc : 0.8413059984813971\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 321/321 [03:34<00:00,  1.50it/s]\n","100%|██████████| 83/83 [00:04<00:00, 17.69it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 8 train_loss: 0.001281607907426643 train_acc: 0.9874121779859485\n","Epoch 8 val_loss:  0.04938582626895183 val_acc : 0.8443432042520881\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 321/321 [03:34<00:00,  1.50it/s]\n","100%|██████████| 83/83 [00:04<00:00, 17.73it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 9 train_loss: 0.001229668036074849 train_acc: 0.9873145979703357\n","Epoch 9 val_loss:  0.04889030825999854 val_acc : 0.8466211085801063\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 321/321 [03:34<00:00,  1.50it/s]\n","100%|██████████| 83/83 [00:04<00:00, 17.54it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 10 train_loss: 0.0008661519027210843 train_acc: 0.9908274785323966\n","Epoch 10 val_loss:  0.056145480706527244 val_acc : 0.8496583143507973\n"]}],"source":["#########################################################################\n","#                          Hyper-parameters                             #\n","#########################################################################\n","max_epoch = 5\n","log_interval = 1\n","best_acc = 0\n","#########################################################################\n","#                          End of your code                             #\n","#########################################################################\n","\n","m = Meter()\n","\n","for epoch in range(1, max_epoch + 1):\n","    train_loss, train_acc = train(model, train_data, optimizer, criterion)\n","    val_loss, val_acc = test(model, val_data, criterion, log_loss=True)\n","\n","    if epoch % log_interval == 0:\n","        print('Epoch {} train_loss: {} train_acc: {}'.format(\n","            epoch, train_loss, train_acc\n","        ))\n","        print('Epoch {} val_loss:  {} val_acc : {}'.format(\n","            epoch, val_loss, val_acc\n","        ))\n","\n","    m.update(train_loss, train_acc, val_loss, val_acc)\n","\n","    # model checkpoint\n","    torch.save(model.state_dict(), 'ckpts/e{}.pt'.format(epoch))\n","    if val_acc > best_acc:\n","        best_model = model\n","        best_acc = val_acc\n","        print('-'*10, 'e', epoch, 'save best model', '-'*10)"]},{"cell_type":"code","execution_count":51,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":843},"executionInfo":{"elapsed":551,"status":"ok","timestamp":1703008091179,"user":{"displayName":"m6","userId":"16445725425625979130"},"user_tz":-480},"id":"SmtW58OR7rIc","outputId":"5d316baf-32d2-48c1-a36f-5452db985c7a"},"outputs":[{"data":{"image/png":"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","text/plain":["<Figure size 640x480 with 1 Axes>"]},"metadata":{},"output_type":"display_data"},{"data":{"image/png":"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","text/plain":["<Figure size 640x480 with 1 Axes>"]},"metadata":{},"output_type":"display_data"}],"source":["# plot them out\n","m.plot()"]},{"cell_type":"markdown","metadata":{"id":"YcJcHf7n7rId"},"source":["# Prediction\n","\n","Predict the results based on testing set. Upload to [Kaggle](https://www.kaggle.com/t/f072e95f51bc48978225941dba218241).\n"]},{"cell_type":"code","execution_count":54,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4245,"status":"ok","timestamp":1703010558580,"user":{"displayName":"m6","userId":"16445725425625979130"},"user_tz":-480},"id":"Lt0Pvt9XSZwn","outputId":"a179ce46-4e9f-4010-dc47-3165d4017549"},"outputs":[{"data":{"text/plain":["<All keys matched successfully>"]},"execution_count":54,"metadata":{},"output_type":"execute_result"}],"source":["best_model = model\n","best_model.load_state_dict(torch.load('ckpts/roberta_8678.pt'))"]},{"cell_type":"code","execution_count":55,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10957,"status":"ok","timestamp":1703010575262,"user":{"displayName":"m6","userId":"16445725425625979130"},"user_tz":-480},"id":"Sf5UTlMZ7rId","outputId":"53510f42-3506-4a36-cb14-7a3f9667a0cd"},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████| 193/193 [00:10<00:00, 17.91it/s]\n"]}],"source":["best_model.eval()\n","\n","total_out = []\n","for text, mask in tqdm(test_data, total=len(test_data)):\n","    text = text.to(device)\n","    mask = mask.to(device)\n","\n","    output = best_model(text, mask)\n","    pred = output.logits\n","    pred = torch.argmax(pred, dim=1)\n","    total_out.append(pred)\n","\n","total_out = torch.cat(total_out).cpu().numpy().tolist()\n","\n","with open('ckpts/pred.csv', 'w') as f:\n","    f.write('index,sentiment_label\\n')\n","    for i, pred in enumerate(total_out):\n","        f.write('{},{}\\n'.format(i, pred))"]},{"cell_type":"markdown","metadata":{"id":"o2ij3BD4R51B"},"source":["# Task 2: In-Context learning (32 points)\n","\n","In this task, you will learn how to perform sentiment classification using **prompts** without the need for training."]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":2685,"status":"ok","timestamp":1703062983919,"user":{"displayName":"m6","userId":"16445725425625979130"},"user_tz":-480},"id":"nXyZ-M7GR51B"},"outputs":[],"source":["import torch\n","import pyprind\n","import numpy as np\n","import pandas as pd\n","import torch.nn as nn\n","from transformers import BertConfig, BertTokenizer, BertForMaskedLM\n","from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score"]},{"cell_type":"markdown","metadata":{"id":"RwpJns8lR51B"},"source":["# Loading model and setup"]},{"cell_type":"code","execution_count":14,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3440,"status":"ok","timestamp":1703063960305,"user":{"displayName":"m6","userId":"16445725425625979130"},"user_tz":-480},"id":"VlwFlir4R51B","outputId":"8df64a63-cb8e-444a-a604-caf5fecbbaab"},"outputs":[{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n","- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n","- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]}],"source":["#########################################################################\n","#         TODO: Design your own template(prefix) and verbalizer         #\n","#########################################################################\n","class Config:\n","    def __init__(self):\n","        # Zero-shot learning template\n","        # self.prefix = \"Sentence for emotion: This sentence is [MASK].\" # you can modify this line\n","\n","        # One-shot\n","        # self.prefix = (\n","        #     \"Sentence for emotion: This sentence is negative. Walking into the office on Monday morning feels like a never-ending struggle, facing a mountain of tasks and deadlines. [SEP] \"\n","        #     \"Sentence for emotion: This sentence is [MASK].\"\n","        # )\n","\n","        # Few-shot\n","        self.prefix = (\n","            \"Sentence for emotion: This sentence is negative. Walking into the office on Monday morning feels like a never-ending struggle, facing a mountain of tasks and deadlines. [SEP] \"\n","            \"Sentence for emotion: This sentence is neutral. Entering the office on Monday morning, I prepare my to-do list, ready to tackle the tasks for the day. [SEP] \"\n","            \"Sentence for emotion: This sentence is positive. Stepping into the office on Monday morning brings a sense of excitement, knowing that new opportunities and achievements await. [SEP] \"\n","            \"Sentence for emotion: This sentence is [MASK].\"\n","        )\n","\n","        self.verbalizer = {\n","            'positive': 2,\n","            'negative': 0,\n","            'neutral': 1\n","        }\n","\n","        self.max_seq_length = 512\n","        self.batch_size = 64\n","\n","\n","config = Config()\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","bert_type = 'bert-base-uncased'\n","\n","model = BertForMaskedLM.from_pretrained(bert_type, num_labels = 3)\n","\n","tokenizer = BertTokenizer.from_pretrained(bert_type)\n","\n","bert_config = BertConfig.from_pretrained(bert_type)\n","\n","bert = model.from_pretrained(bert_type, config=bert_config).to(device)\n","\n","#######################################################################\n","#                        End of your code                             #\n","#######################################################################\n","\n","softmax = nn.Softmax(dim=1)"]},{"cell_type":"markdown","metadata":{"id":"oo-rRxPMR51C"},"source":["## Obtaion verbalizer ids"]},{"cell_type":"code","execution_count":15,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1703063960305,"user":{"displayName":"m6","userId":"16445725425625979130"},"user_tz":-480},"id":"Eo1fvWLjR51G"},"outputs":[],"source":["# Utility function to obtaion verbalizer ids\n","def obtain_verbalizer_ids(verbalizer, tokenizer):\n","    verbalizer_ids = tokenizer.convert_tokens_to_ids(list(verbalizer.keys()))\n","    index2ids = {i: verbalizer_ids[i] for i in range(len(verbalizer_ids))}\n","    return verbalizer_ids, index2ids\n"]},{"cell_type":"code","execution_count":16,"metadata":{"executionInfo":{"elapsed":1,"status":"ok","timestamp":1703063961675,"user":{"displayName":"m6","userId":"16445725425625979130"},"user_tz":-480},"id":"1F7jpPtVR51G"},"outputs":[],"source":["verbalizer_ids, index2ids = obtain_verbalizer_ids(config.verbalizer, tokenizer)"]},{"cell_type":"markdown","metadata":{"id":"oD-hP6-2R51G"},"source":["## Concatenate original text and prefix"]},{"cell_type":"code","execution_count":18,"metadata":{"executionInfo":{"elapsed":246,"status":"ok","timestamp":1703063990440,"user":{"displayName":"m6","userId":"16445725425625979130"},"user_tz":-480},"id":"WGY_xVYfR51G"},"outputs":[],"source":["# Utility function to concatenate prefix and text\n","def concatenate_prefix(texts, config):\n","    ##################################################\n","    #   TODO: concatenate your own prefix and text   #\n","    ##################################################\n","    prefix_texts = [config.prefix + \" \" + text for text in texts]\n","    # prefix_texts = [config.prefix.format(text=text) for text in texts]\n","    ##################################################\n","    #                 End of your code               #\n","    ##################################################\n","    return prefix_texts"]},{"cell_type":"code","execution_count":19,"metadata":{"executionInfo":{"elapsed":433,"status":"ok","timestamp":1703063991937,"user":{"displayName":"m6","userId":"16445725425625979130"},"user_tz":-480},"id":"VPqjVt6PR51G"},"outputs":[],"source":["def load_data(config):\n","    # ['texts', 'labels']\n","    df = pd.read_csv('./twitter_sentiment/train.csv')\n","    original_texts = df['text'].tolist()\n","    labels = df['sentiment_label'].tolist()\n","\n","    texts = concatenate_prefix(original_texts, config)\n","\n","    return texts, labels\n","\n","\n","texts, labels = load_data(config)"]},{"cell_type":"code","execution_count":20,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1703063993333,"user":{"displayName":"m6","userId":"16445725425625979130"},"user_tz":-480},"id":"axTNaySxR51G"},"outputs":[],"source":["# Batching of texts and labels for training or processing in batches\n","def pack_batch(texts, labels, batch_size):\n","    \"\"\"\n","    :param texts: list\n","    :param labels: list\n","    :param batch_size: int\n","    :return batch_X: list\n","            [[text11, text12, ...], [text21, text22, ...], ...]\n","    :return batch_y: list\n","            [[label11, label12, ...], [label21, label22, ...], ...]\n","    :return batch_count: int\n","    \"\"\"\n","    assert len(texts) == len(labels)\n","\n","    if len(texts) % batch_size != 0:\n","        flag = False\n","        batch_count = int(len(texts) / batch_size) + 1\n","    else:\n","        flag = True\n","        batch_count = int(len(texts) / batch_size)\n","\n","    batch_X, batch_y = [], []\n","\n","    if flag:\n","        for i in range(batch_count):\n","            batch_X.append(texts[i * batch_size: (i + 1) * batch_size])\n","            batch_y.append(labels[i * batch_size: (i + 1) * batch_size])\n","    else:\n","        for i in range(batch_count):\n","            if i == batch_count - 1:\n","                batch_X.append(texts[i * batch_size:])\n","                batch_y.append(labels[i * batch_size:])\n","            else:\n","                batch_X.append(texts[i * batch_size: (i + 1) * batch_size])\n","                batch_y.append(labels[i * batch_size: (i + 1) * batch_size])\n","\n","    return batch_X, batch_y, batch_count"]},{"cell_type":"code","execution_count":21,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1703063995537,"user":{"displayName":"m6","userId":"16445725425625979130"},"user_tz":-480},"id":"hGqHvhRsR51H"},"outputs":[],"source":["batch_X, batch_y, batch_count = pack_batch(texts, labels, config.batch_size)"]},{"cell_type":"markdown","metadata":{"id":"UEjcoyWoR51H"},"source":["## Inferencing the model without training"]},{"cell_type":"code","execution_count":22,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":446286,"status":"ok","timestamp":1703064442619,"user":{"displayName":"m6","userId":"16445725425625979130"},"user_tz":-480},"id":"WT_rYVZ3R51H","outputId":"d7fe28fa-0093-4efe-d554-68d145b0f840"},"outputs":[{"name":"stderr","output_type":"stream","text":["[100 %] Time elapsed: 00:07:26 | ETA: 00:00:00"]},{"name":"stdout","output_type":"stream","text":["\n"," accuracy: 0.220141 | precision: 0.653974 | recall: 0.220141 | f1: 0.155925\n"]},{"name":"stderr","output_type":"stream","text":["\n","Total time elapsed: 00:07:26\n"]}],"source":["with torch.no_grad():\n","    predict_all = np.array([], dtype=int)\n","    labels_all = np.array([], dtype=int)\n","    pper = pyprind.ProgPercent(batch_count)\n","    for i in range(batch_count):\n","        inputs = batch_X[i]\n","        labels = batch_y[i]\n","\n","        # Using the BERT tokenizer (tokenizer.batch_encode_plus), adding special tokens, ensuring a maximum sequence length, and handling padding/truncation\n","        tokens = tokenizer.batch_encode_plus(inputs, add_special_tokens=True,\n","                                             max_length=config.max_seq_length,\n","                                             padding='max_length', truncation=True)\n","\n","        ids = torch.tensor(tokens['input_ids']).to(device)\n","        attention_mask = torch.tensor(tokens['attention_mask']).to(device)\n","\n","        # Shape: (batch_size, max_seq_length, vocab_size)\n","        logits = bert(ids, attention_mask=attention_mask).logits\n","\n","        mask_token_index = (ids == tokenizer.mask_token_id).nonzero(as_tuple=True)\n","\n","        # Find [MASK] logits\n","        # shape: (batch_size, vocab_size)\n","        masked_logits = logits[mask_token_index[0], mask_token_index[1], :]\n","\n","        # Extract the logits of the word in the verbalizer at the [MASK] position\n","        # shape: (batch_size, verbalizer_size)\n","        verbalizer_logits = masked_logits[:, verbalizer_ids]\n","\n","        # Construct a pseudo-distribution from the logits in these verbalizers\n","        pseudo_distribution = softmax(verbalizer_logits)\n","\n","        #################################################################################\n","        #   1. Find the index with the maximum probability in the pseudo-distribution   #\n","        #   2. Convert the index to the corresponding word ID                           #\n","        #   3. Convert the ID to a token                                                #\n","        #   4. Find the label corresponding to the token                                #\n","        #################################################################################\n","\n","        pred_indices = torch.argmax(pseudo_distribution, dim=1)\n","\n","        pred_ids = [verbalizer_ids[index] for index in pred_indices]\n","\n","        pred_tokens = tokenizer.convert_ids_to_tokens(pred_ids)\n","\n","        pred_labels = [config.verbalizer[token] if token in config.verbalizer else -1 for token in pred_tokens]\n","        pred_labels = np.array(pred_labels)\n","        #################################################################################\n","        #                             End of your code                                  #\n","        #################################################################################\n","\n","        predict_all = np.append(predict_all, pred_labels)\n","        labels_all = np.append(labels_all, labels)\n","\n","        pper.update()\n","\n","    acc = accuracy_score(labels_all, predict_all)\n","    p = precision_score(labels_all, predict_all, average=\"weighted\")\n","    r = recall_score(labels_all, predict_all, average=\"weighted\")\n","    f1 = f1_score(labels_all, predict_all, average=\"weighted\")\n","\n","    print('\\n accuracy: %f | precision: %f | recall: %f | f1: %f' % (acc, p, r, f1))"]},{"cell_type":"markdown","metadata":{"id":"qE5OCs8NR51H"},"source":["# Task 3: LM-BFF (45 points)\n","\n","https://arxiv.org/pdf/2012.15723.pdf\n","\n","Unlike the previous task, LM-BFF can generate templates and verbalizers automatically."]},{"cell_type":"markdown","metadata":{"id":"kushn7bvR51I"},"source":["# Get Data"]},{"cell_type":"markdown","metadata":{"id":"ADH0TYMxR51I"},"source":["請先到共用雲端硬碟將檔案 `SST-2.zip`，建立捷徑到自己的雲端硬碟中。\n","\n","> 操作步驟\n","1. 點開雲端[連結](https://drive.google.com/file/d/14MDYFasXU94dUE9DjgfcZE61iTRI2007/view?usp=sharing)\n","2. 點選右上角「新增雲端硬碟捷徑」\n","3. 點選「我的雲端硬碟」\n","4. 點選「新增捷徑」\n","\n","完成以上流程會在你的雲端硬碟中建立一個檔案的捷徑，接著我們在colab中取得權限即可使用。"]},{"cell_type":"markdown","metadata":{"id":"kE2urpvrR51I"},"source":["# Install openprompt\n","\n","This library provides a standard, flexible and extensible framework to deploy the prompt-learning pipeline.\n","\n","[OpenPrompt Documentation](https://thunlp.github.io/OpenPrompt/)"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":21050,"status":"ok","timestamp":1703078537412,"user":{"displayName":"m6","userId":"16445725425625979130"},"user_tz":-480},"id":"sjULW5RNR51I","outputId":"3d53749c-8ad9-4c16-ac9f-98294796004f"},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting openprompt\n","  Downloading openprompt-1.0.1-py3-none-any.whl (146 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m146.4/146.4 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: transformers>=4.10.0 in /usr/local/lib/python3.10/dist-packages (from openprompt) (4.35.2)\n","Collecting sentencepiece==0.1.96 (from openprompt)\n","  Downloading sentencepiece-0.1.96-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: tqdm>=4.62.2 in /usr/local/lib/python3.10/dist-packages (from openprompt) (4.66.1)\n","Collecting tensorboardX (from openprompt)\n","  Downloading tensorboardX-2.6.2.2-py2.py3-none-any.whl (101 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.7/101.7 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from openprompt) (3.8.1)\n","Collecting yacs (from openprompt)\n","  Downloading yacs-0.1.8-py3-none-any.whl (14 kB)\n","Collecting dill (from openprompt)\n","  Downloading dill-0.3.7-py3-none-any.whl (115 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting datasets (from openprompt)\n","  Downloading datasets-2.15.0-py3-none-any.whl (521 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m521.2/521.2 kB\u001b[0m \u001b[31m18.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting rouge==1.0.0 (from openprompt)\n","  Downloading rouge-1.0.0-py3-none-any.whl (14 kB)\n","Requirement already satisfied: pyarrow in /usr/local/lib/python3.10/dist-packages (from openprompt) (10.0.1)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from openprompt) (1.11.4)\n","Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from rouge==1.0.0->openprompt) (1.16.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers>=4.10.0->openprompt) (3.13.1)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.10.0->openprompt) (0.19.4)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.10.0->openprompt) (1.23.5)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.10.0->openprompt) (23.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.10.0->openprompt) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.10.0->openprompt) (2023.6.3)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers>=4.10.0->openprompt) (2.31.0)\n","Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.10.0->openprompt) (0.15.0)\n","Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.10.0->openprompt) (0.4.1)\n","Collecting pyarrow-hotfix (from datasets->openprompt)\n","  Downloading pyarrow_hotfix-0.6-py3-none-any.whl (7.9 kB)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets->openprompt) (1.5.3)\n","Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets->openprompt) (3.4.1)\n","Collecting multiprocess (from datasets->openprompt)\n","  Downloading multiprocess-0.70.15-py310-none-any.whl (134 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: fsspec[http]<=2023.10.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets->openprompt) (2023.6.0)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets->openprompt) (3.9.1)\n","Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->openprompt) (8.1.7)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->openprompt) (1.3.2)\n","Requirement already satisfied: protobuf>=3.20 in /usr/local/lib/python3.10/dist-packages (from tensorboardX->openprompt) (3.20.3)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->openprompt) (23.1.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->openprompt) (6.0.4)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->openprompt) (1.9.4)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->openprompt) (1.4.0)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->openprompt) (1.3.1)\n","Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->openprompt) (4.0.3)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers>=4.10.0->openprompt) (4.5.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.10.0->openprompt) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.10.0->openprompt) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.10.0->openprompt) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.10.0->openprompt) (2023.11.17)\n","Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->openprompt) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->openprompt) (2023.3.post1)\n","Installing collected packages: sentencepiece, yacs, tensorboardX, rouge, pyarrow-hotfix, dill, multiprocess, datasets, openprompt\n","Successfully installed datasets-2.15.0 dill-0.3.7 multiprocess-0.70.15 openprompt-1.0.1 pyarrow-hotfix-0.6 rouge-1.0.0 sentencepiece-0.1.96 tensorboardX-2.6.2.2 yacs-0.1.8\n"]}],"source":["!pip install openprompt"]},{"cell_type":"markdown","metadata":{"id":"dKbEzHEoR51I"},"source":["# Import openprompt package"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":11715,"status":"ok","timestamp":1703078549124,"user":{"displayName":"m6","userId":"16445725425625979130"},"user_tz":-480},"id":"qSatBBhGR51I","outputId":"e7d6efda-ac05-427a-ed29-86548fef6a10"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/transformers/generation_utils.py:24: FutureWarning: Importing `GenerationMixin` from `src/transformers/generation_utils.py` is deprecated and will be removed in Transformers v5. Import as `from transformers import GenerationMixin` instead.\n","  warnings.warn(\n"]}],"source":["from openprompt.plms import load_plm\n","from openprompt.prompts.prompt_generator import T5TemplateGenerator\n","from openprompt.pipeline_base import PromptDataLoader, PromptForClassification\n","from openprompt.prompts import ManualTemplate\n","from openprompt.trainer import ClassificationRunner\n","import copy\n","import torch\n","from transformers import  AdamW, get_linear_schedule_with_warmup\n","import numpy as np"]},{"cell_type":"markdown","metadata":{"id":"IUNrCuUKR51I"},"source":["# Setup cuda and whether to perform automatic generation"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1703078549124,"user":{"displayName":"m6","userId":"16445725425625979130"},"user_tz":-480},"id":"WZdINwUsR51I"},"outputs":[],"source":["cuda = True\n","auto_t = True # Whether to perform automatic template generation\n","auto_v = True # Whether to perform automatic verbalizer generation"]},{"cell_type":"markdown","metadata":{"id":"A9FprOUSR51J"},"source":["# Load dataset and model"]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":1229,"status":"ok","timestamp":1703078699172,"user":{"displayName":"m6","userId":"16445725425625979130"},"user_tz":-480},"id":"jUvvIEAwZU_V"},"outputs":[],"source":["# !unzip -qq ./drive/MyDrive/Deep_Learning/A6/SST-2.zip -d ./drive/MyDrive/Deep_Learning/A6"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":594,"status":"ok","timestamp":1703078760467,"user":{"displayName":"m6","userId":"16445725425625979130"},"user_tz":-480},"id":"OXAR6cfhZZkh","outputId":"00db6dcb-2c2a-4830-a6dd-4c1590c5b7d9"},"outputs":[{"name":"stdout","output_type":"stream","text":["新的當前工作目錄： /content/drive/MyDrive/Deep_Learning/A6\n"]}],"source":["import os\n","# 要切換的目錄\n","new_directory = '/content/drive/MyDrive/Deep_Learning/A6'\n","\n","# 更改當前工作目錄\n","os.chdir(new_directory)\n","\n","# 更新當前工作目錄\n","current_path = os.getcwd()\n","print(\"新的當前工作目錄：\", current_path)"]},{"cell_type":"code","execution_count":10,"metadata":{"executionInfo":{"elapsed":580,"status":"ok","timestamp":1703078880600,"user":{"displayName":"m6","userId":"16445725425625979130"},"user_tz":-480},"id":"sMtiZAIkR51J"},"outputs":[],"source":["from openprompt.data_utils.text_classification_dataset import SST2Processor\n","dataset = {}\n","dataset['train'] = SST2Processor().get_train_examples(\"SST-2/\")\n","dataset['validation'] = SST2Processor().get_dev_examples(\"SST-2/\")\n","dataset['test'] = SST2Processor().get_test_examples(\"SST-2/\")"]},{"cell_type":"code","execution_count":19,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":51104,"status":"ok","timestamp":1703084914784,"user":{"displayName":"m6","userId":"16445725425625979130"},"user_tz":-480},"id":"PzRe2b-0Ypka","outputId":"7103a220-cd5f-4fc3-942c-ea0a05ff0c58"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/transformers/models/t5/tokenization_t5.py:240: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n","For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n","- Be aware that you SHOULD NOT rely on t5-large automatically truncating your input to 512 when padding/encoding.\n","- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n","- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n","  warnings.warn(\n","Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"]},{"name":"stdout","output_type":"stream","text":["dataset: {\n","  \"guid\": \"train-0\",\n","  \"label\": 0,\n","  \"meta\": {\n","    \"labelword\": \"terrible\"\n","  },\n","  \"text_a\": \"nothing happens , and it happens to flat characters .\",\n","  \"text_b\": \"\",\n","  \"tgt_text\": null\n","}\n","\n"]}],"source":["#print('load model...')\n","from openprompt.plms import load_plm\n","# load mlm model for main tasks\n","plm, tokenizer, model_config, WrapperClass = load_plm(\"roberta\", \"roberta-large\")\n","\n","# load generation model for template generation\n","template_generate_model, template_generate_tokenizer, template_generate_model_config, template_tokenizer_wrapper = load_plm('t5', 't5-large')\n","\n","from openprompt.prompts import ManualVerbalizer, ManualTemplate\n","\n","\n","###################################################################################################################\n","#   TODO: You need to switch LMBFFTemplateGenerationTemplate or ManualTemplate to                                 #\n","#         compare auto generate template and manual generate template                                             #\n","###################################################################################################################\n","from openprompt.prompts.prompt_generator import LMBFFTemplateGenerationTemplate\n","########################################\n","#   LMBFFTemplateGenerationTemplate    #\n","########################################\n","import random\n","\n","if auto_t:\n","    verbalizer = ManualVerbalizer(tokenizer=tokenizer, num_classes=2, label_words=['terrible','great'])\n","\n","    # number of demonstrations\n","    num_demonstrations = 1  # try different number\n","\n","    demonstrations = []\n","\n","    for _ in range(num_demonstrations):\n","        # random choice training set example with label 0\n","        random_example_1 = random.choice([example for example in dataset['train'] if example.label == 0])\n","\n","        # random choice training set example with label 1\n","        random_example_2 = random.choice([example for example in dataset['train'] if example.label == 1])\n","\n","        demonstration = f'{random_example_1.text_a} It was terrible. {random_example_2.text_a} It was great.'\n","        demonstrations.append(demonstration)\n","\n","    # You can modify the demonstrations and try different combinations\n","    template_text = '{\"placeholder\": \"text_a\"} {\"mask\"} {\"meta\": \"labelword\"} {\"mask\"}.' + ' '.join(demonstrations)\n","    template = LMBFFTemplateGenerationTemplate(tokenizer=template_generate_tokenizer, verbalizer=verbalizer, text=template_text)\n","#############################################\n","#   End of LMBFFTemplateGenerationTemplate  #\n","#############################################\n","\n","########################################\n","#          ManualTemplate              #\n","########################################\n","else:\n","    # number of demonstrations\n","    num_demonstrations = 1  # try different number\n","\n","    demonstrations = []\n","\n","    for _ in range(num_demonstrations):\n","        # random choice training set example with label 0\n","        random_example_1 = random.choice([example for example in dataset['train'] if example.label == 0])\n","\n","        # random choice training set example with label 1\n","        random_example_2 = random.choice([example for example in dataset['train'] if example.label == 1])\n","\n","        demonstration = f'{random_example_1.text_a} Sentence for analyze: This sentence is negative. {random_example_2.text_a} Sentence for analyze: This sentence is positive.'\n","        demonstrations.append(demonstration)\n","\n","    # ManualTemplate 1\n","    # template = ManualTemplate(tokenizer=tokenizer, text='{\"placeholder\":\"text_a\"} Sentence for analyze: This sentence is {\"mask\"}.' + ' '.join(demonstrations))\n","    # verbalizer = ManualVerbalizer(tokenizer=tokenizer, num_classes=2, label_words=['negative','positive'])\n","\n","    # ManualTemplate 2\n","    template = ManualTemplate(tokenizer=tokenizer, text='{\"placeholder\":\"text_a\"} The emotion conveyed in this text is {\"mask\"}.' + ' '.join(demonstrations))\n","    verbalizer = ManualVerbalizer(tokenizer=tokenizer, num_classes=2, label_words=['horrible','wonderful'])\n","\n","    # ManualTemplate 3\n","    template = ManualTemplate(tokenizer=tokenizer, text='{\"placeholder\":\"text_a\"} It was {\"mask\"} sentence.' + ' '.join(demonstrations))\n","    verbalizer = ManualVerbalizer(tokenizer=tokenizer, num_classes=2, label_words=['bad','good'])\n","\n","#############################################\n","#          End of ManualTemplate            #\n","#############################################\n","\n","###################################################################################################################\n","#                                           End of your code                                                      #\n","###################################################################################################################\n","\n","\n","# view wrapped example\n","wrapped_example = template.wrap_one_example(dataset['train'][0])\n","print(\"dataset:\", dataset['train'][0])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zmEVD03OR51J"},"outputs":[],"source":["# #print('load model...')\n","# from openprompt.plms import load_plm\n","\n","# # load mlm model for main tasks\n","# plm, tokenizer, model_config, WrapperClass = load_plm(\"roberta\", \"roberta-large\")\n","\n","# # load generation model for template generation\n","# template_generate_model, template_generate_tokenizer, template_generate_model_config, template_tokenizer_wrapper = load_plm('t5', 't5-large')\n","\n","# from openprompt.prompts import ManualVerbalizer, ManualTemplate\n","\n","# # if you wish to do automatic label word generation, the verbalizer is not the final verbalizer, and is only used for template generation.\n","# verbalizer = ManualVerbalizer(tokenizer=tokenizer, num_classes=2, label_words=[['terrible'],['great']]) # Manually generate the verbalizer\n","\n","\n","# ###################################################################################################################\n","# #   TODO: You need to switch LMBFFTemplateGenerationTemplate or ManualTemplate to                                 #\n","# #         compare auto generate template and manual generate template                                             #\n","# ###################################################################################################################\n","# from openprompt.prompts.prompt_generator import LMBFFTemplateGenerationTemplate\n","\n","# ############################################\n","# #   LMBFFTemplateGenerationTemplate        #\n","# ############################################\n","# import random\n","\n","# # number of demonstrations\n","# num_demonstrations = 1  # try different number\n","\n","# demonstrations = []\n","\n","# for _ in range(num_demonstrations):\n","#     # random choice training set example with label 0\n","#     random_example_1 = random.choice([example for example in dataset['train'] if example.label == 0])\n","\n","#     # random choice training set example with label 1\n","#     random_example_2 = random.choice([example for example in dataset['train'] if example.label == 1])\n","\n","#     demonstration = f'{random_example_1.text_a} It was terrible. {random_example_2.text_a} It was great.'\n","#     demonstrations.append(demonstration)\n","\n","# # You can modify the demonstrations and try different combinations\n","# template_text = '{\"placeholder\": \"text_a\"} {\"mask\"} {\"meta\": \"labelword\"} {\"mask\"}.' + ' '.join(demonstrations)\n","# template = LMBFFTemplateGenerationTemplate(tokenizer=template_generate_tokenizer, verbalizer=verbalizer, text=template_text)\n","\n","# #############################################\n","# #   End of LMBFFTemplateGenerationTemplate  #\n","# #############################################\n","\n","# ########################################\n","# #          ManualTemplate              #\n","# ########################################\n","\n","# template = ManualTemplate(tokenizer=tokenizer, text='{\"placeholder\":\"text_a\"} It was {\"mask\"}.')\n","\n","# ########################################\n","# #          End of ManualTemplate       #\n","# ########################################\n","\n","# ###################################################################################################################\n","# #                                           End of your code                                                      #\n","# ###################################################################################################################\n","\n","\n","# # view wrapped example\n","# wrapped_example = template.wrap_one_example(dataset['train'][0])\n","# print(\"dataset:\", dataset['train'][0])\n"]},{"cell_type":"markdown","metadata":{"id":"K1hBixRQR51J"},"source":["# Utility Function"]},{"cell_type":"code","execution_count":20,"metadata":{"executionInfo":{"elapsed":1023,"status":"ok","timestamp":1703084942255,"user":{"displayName":"m6","userId":"16445725425625979130"},"user_tz":-480},"id":"yIFOzMsRR51J"},"outputs":[],"source":["from openprompt.plms import load_plm\n","from openprompt.prompts.prompt_generator import T5TemplateGenerator\n","from openprompt.pipeline_base import PromptDataLoader, PromptForClassification\n","from openprompt.prompts import ManualTemplate\n","from openprompt.trainer import ClassificationRunner\n","import copy\n","import torch\n","from transformers import  AdamW, get_linear_schedule_with_warmup\n","import numpy as np\n","\n","# Returns the best evaluation score achieved during training\n","def fit(model, train_dataloader, val_dataloader, loss_func, optimizer):\n","    best_score = 0.0\n","    for epoch in range(5):\n","        train_loss = train_epoch(model, train_dataloader, loss_func, optimizer)\n","        score = evaluate(model, val_dataloader)\n","        if score > best_score:\n","            best_score = score\n","        print(f\"Epoch {epoch+1}: Train loss={train_loss}, Eval score={score}\")\n","    return best_score\n","\n","# Trains the model on the training data and computes the training loss\n","def train_epoch(model, train_dataloader, loss_func, optimizer):\n","    model.train()\n","    loss_all = []\n","    for step, inputs in enumerate(train_dataloader):\n","        if cuda:\n","            inputs = inputs.cuda()\n","        #####################################################\n","        # 1. Put correct variables into model to get logits #\n","        # 2. Get labels                                     #\n","        # 3. Evalutate using loss_func                         #\n","        # 4. Append loss to loss_all                        #\n","        #####################################################\n","        logits = model(batch=inputs)\n","        labels = inputs['label']\n","        loss = loss_func(logits, labels)\n","        loss.backward()\n","        loss_all.append(loss.item())\n","        #####################################################\n","        #                 End of your code                  #\n","        #####################################################\n","        optimizer.step()\n","        optimizer.zero_grad()\n","    return np.mean(loss_all)\n","\n","def evaluate(model, val_dataloader):\n","    model.eval()\n","    allpreds = []\n","    alllabels = []\n","    with torch.no_grad():\n","        for step, inputs in enumerate(val_dataloader):\n","            if cuda:\n","                inputs = inputs.cuda()\n","            #####################################################\n","            # 1. Put correct variables into model to get logits #\n","            # 2. Get labels                                     #\n","            # 3. Extend labels to list                          #\n","            # 4. Get predictions and extend preds to list        #\n","            #####################################################\n","            logits = model(batch=inputs)\n","\n","            labels = inputs['label']\n","\n","            alllabels.extend(labels.cpu().numpy())\n","            # 4. Get predictions and extend preds to list\n","            preds = torch.argmax(logits, dim=1)\n","            allpreds.extend(preds.cpu().numpy())\n","            #####################################################\n","            #                 End of your code                  #\n","            #####################################################\n","    acc = sum([int(i==j) for i,j in zip(allpreds, alllabels)])/len(allpreds)\n","    return acc"]},{"cell_type":"markdown","metadata":{"id":"egrqvnhNR51J"},"source":["# Automatic template generation"]},{"cell_type":"markdown","metadata":{"id":"KRtKm-8dR51K"},"source":["Generated template from TemplateGenerator and find the best template"]},{"cell_type":"code","execution_count":21,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":535855,"status":"ok","timestamp":1703085482469,"user":{"displayName":"m6","userId":"16445725425625979130"},"user_tz":-480},"id":"wjhGuiDIR51K","outputId":"33efdd36-e29d-487c-83b4-2e11c9c99d18"},"outputs":[{"name":"stdout","output_type":"stream","text":["performing auto_t...\n"]},{"name":"stderr","output_type":"stream","text":["tokenizing: 32it [00:00, 773.88it/s]\n"]},{"name":"stdout","output_type":"stream","text":["generating...\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 18/18 [00:51<00:00,  2.85s/it]\n"]},{"name":"stdout","output_type":"stream","text":["['{\"placeholder\": \"text_a\"} It was {\"mask\"} ..the plot is nothing but boilerplate clichés from start to finish , and the script assumes that not only would subtlety be lost on the target audience , but that it \\'s also too stupid to realize that they \\'ve already seen this exact same movie a hundred times It was terrible. the film presents visceral and dangerously honest revelations about the men and machines behind the curtains of our planet . It was great.', '{\"placeholder\": \"text_a\"} it was {\"mask\"} ..the plot is nothing but boilerplate clichés from start to finish , and the script assumes that not only would subtlety be lost on the target audience , but that it \\'s also too stupid to realize that they \\'ve already seen this exact same movie a hundred times It was terrible. the film presents visceral and dangerously honest revelations about the men and machines behind the curtains of our planet . It was great.', '{\"placeholder\": \"text_a\"} It was {\"mask\"} . It was terrible.the plot is nothing but boilerplate clichés from start to finish , and the script assumes that not only would subtlety be lost on the target audience , but that it \\'s also too stupid to realize that they \\'ve already seen this exact same movie a hundred times It was terrible. the film presents visceral and dangerously honest revelations about the men and machines behind the curtains of our planet . It was great.', '{\"placeholder\": \"text_a\"} . It was {\"mask\"} ..the plot is nothing but boilerplate clichés from start to finish , and the script assumes that not only would subtlety be lost on the target audience , but that it \\'s also too stupid to realize that they \\'ve already seen this exact same movie a hundred times It was terrible. the film presents visceral and dangerously honest revelations about the men and machines behind the curtains of our planet . It was great.', '{\"placeholder\": \"text_a\"} It was {\"mask\"} . It was horrible.the plot is nothing but boilerplate clichés from start to finish , and the script assumes that not only would subtlety be lost on the target audience , but that it \\'s also too stupid to realize that they \\'ve already seen this exact same movie a hundred times It was terrible. the film presents visceral and dangerously honest revelations about the men and machines behind the curtains of our planet . It was great.']\n"]},{"name":"stderr","output_type":"stream","text":["\r  0%|          | 0/5 [00:00<?, ?it/s]"]},{"name":"stdout","output_type":"stream","text":["current template: {\"placeholder\": \"text_a\"} It was {\"mask\"} ..the plot is nothing but boilerplate clichés from start to finish , and the script assumes that not only would subtlety be lost on the target audience , but that it 's also too stupid to realize that they 've already seen this exact same movie a hundred times It was terrible. the film presents visceral and dangerously honest revelations about the men and machines behind the curtains of our planet . It was great., wrapped example: [[{'text': 'nothing happens , and it happens to flat characters .', 'loss_ids': 0, 'shortenable_ids': 1}, {'text': ' It was', 'loss_ids': 0, 'shortenable_ids': 0}, {'text': '<mask>', 'loss_ids': 1, 'shortenable_ids': 0}, {'text': \" ..the plot is nothing but boilerplate clichés from start to finish , and the script assumes that not only would subtlety be lost on the target audience , but that it 's also too stupid to realize that they 've already seen this exact same movie a hundred times It was terrible. the film presents visceral and dangerously honest revelations about the men and machines behind the curtains of our planet . It was great.\", 'loss_ids': 0, 'shortenable_ids': 0}], {'guid': 'train-0', 'label': 0}]\n"]},{"name":"stderr","output_type":"stream","text":["\n","tokenizing: 32it [00:00, 540.62it/s]\n","\n","tokenizing: 32it [00:00, 628.99it/s]\n","/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 1: Train loss=1.4726448110304773, Eval score=0.6875\n","Epoch 2: Train loss=0.4943479045814456, Eval score=0.75\n","Epoch 3: Train loss=0.5817366527899139, Eval score=0.5\n","Epoch 4: Train loss=0.9098197789862752, Eval score=0.625\n"]},{"name":"stderr","output_type":"stream","text":["\r 20%|██        | 1/5 [01:36<06:26, 96.67s/it]"]},{"name":"stdout","output_type":"stream","text":["Epoch 5: Train loss=0.37653766002040356, Eval score=0.8125\n","current template: {\"placeholder\": \"text_a\"} it was {\"mask\"} ..the plot is nothing but boilerplate clichés from start to finish , and the script assumes that not only would subtlety be lost on the target audience , but that it 's also too stupid to realize that they 've already seen this exact same movie a hundred times It was terrible. the film presents visceral and dangerously honest revelations about the men and machines behind the curtains of our planet . It was great., wrapped example: [[{'text': 'nothing happens , and it happens to flat characters .', 'loss_ids': 0, 'shortenable_ids': 1}, {'text': ' it was', 'loss_ids': 0, 'shortenable_ids': 0}, {'text': '<mask>', 'loss_ids': 1, 'shortenable_ids': 0}, {'text': \" ..the plot is nothing but boilerplate clichés from start to finish , and the script assumes that not only would subtlety be lost on the target audience , but that it 's also too stupid to realize that they 've already seen this exact same movie a hundred times It was terrible. the film presents visceral and dangerously honest revelations about the men and machines behind the curtains of our planet . It was great.\", 'loss_ids': 0, 'shortenable_ids': 0}], {'guid': 'train-0', 'label': 0}]\n"]},{"name":"stderr","output_type":"stream","text":["\n","tokenizing: 32it [00:00, 570.84it/s]\n","\n","tokenizing: 32it [00:00, 654.79it/s]\n","/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 1: Train loss=1.9111376700993785, Eval score=0.5625\n","Epoch 2: Train loss=0.7908010957762599, Eval score=0.65625\n","Epoch 3: Train loss=0.3268858352457755, Eval score=0.65625\n","Epoch 4: Train loss=0.008014625260784669, Eval score=0.6875\n"]},{"name":"stderr","output_type":"stream","text":["\r 40%|████      | 2/5 [03:13<04:49, 96.56s/it]"]},{"name":"stdout","output_type":"stream","text":["Epoch 5: Train loss=0.0010577958761786022, Eval score=0.6875\n","current template: {\"placeholder\": \"text_a\"} It was {\"mask\"} . It was terrible.the plot is nothing but boilerplate clichés from start to finish , and the script assumes that not only would subtlety be lost on the target audience , but that it 's also too stupid to realize that they 've already seen this exact same movie a hundred times It was terrible. the film presents visceral and dangerously honest revelations about the men and machines behind the curtains of our planet . It was great., wrapped example: [[{'text': 'nothing happens , and it happens to flat characters .', 'loss_ids': 0, 'shortenable_ids': 1}, {'text': ' It was', 'loss_ids': 0, 'shortenable_ids': 0}, {'text': '<mask>', 'loss_ids': 1, 'shortenable_ids': 0}, {'text': \" . It was terrible.the plot is nothing but boilerplate clichés from start to finish , and the script assumes that not only would subtlety be lost on the target audience , but that it 's also too stupid to realize that they 've already seen this exact same movie a hundred times It was terrible. the film presents visceral and dangerously honest revelations about the men and machines behind the curtains of our planet . It was great.\", 'loss_ids': 0, 'shortenable_ids': 0}], {'guid': 'train-0', 'label': 0}]\n"]},{"name":"stderr","output_type":"stream","text":["\n","tokenizing: 32it [00:00, 673.34it/s]\n","\n","tokenizing: 32it [00:00, 721.27it/s]\n","/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 1: Train loss=1.7873651112786302, Eval score=0.5\n","Epoch 2: Train loss=0.8661239664070308, Eval score=0.5625\n","Epoch 3: Train loss=0.8419872804079205, Eval score=0.8125\n","Epoch 4: Train loss=0.6803810726851225, Eval score=0.78125\n"]},{"name":"stderr","output_type":"stream","text":["\r 60%|██████    | 3/5 [04:49<03:13, 96.52s/it]"]},{"name":"stdout","output_type":"stream","text":["Epoch 5: Train loss=0.5007358799921349, Eval score=0.5\n","current template: {\"placeholder\": \"text_a\"} . It was {\"mask\"} ..the plot is nothing but boilerplate clichés from start to finish , and the script assumes that not only would subtlety be lost on the target audience , but that it 's also too stupid to realize that they 've already seen this exact same movie a hundred times It was terrible. the film presents visceral and dangerously honest revelations about the men and machines behind the curtains of our planet . It was great., wrapped example: [[{'text': 'nothing happens , and it happens to flat characters .', 'loss_ids': 0, 'shortenable_ids': 1}, {'text': ' . It was', 'loss_ids': 0, 'shortenable_ids': 0}, {'text': '<mask>', 'loss_ids': 1, 'shortenable_ids': 0}, {'text': \" ..the plot is nothing but boilerplate clichés from start to finish , and the script assumes that not only would subtlety be lost on the target audience , but that it 's also too stupid to realize that they 've already seen this exact same movie a hundred times It was terrible. the film presents visceral and dangerously honest revelations about the men and machines behind the curtains of our planet . It was great.\", 'loss_ids': 0, 'shortenable_ids': 0}], {'guid': 'train-0', 'label': 0}]\n"]},{"name":"stderr","output_type":"stream","text":["\n","tokenizing: 32it [00:00, 687.56it/s]\n","\n","tokenizing: 32it [00:00, 671.59it/s]\n","/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 1: Train loss=1.5780276267323643, Eval score=0.5\n","Epoch 2: Train loss=0.7432095754193142, Eval score=0.5\n","Epoch 3: Train loss=1.5140164698823355, Eval score=0.5\n","Epoch 4: Train loss=0.7962554381228983, Eval score=0.5\n"]},{"name":"stderr","output_type":"stream","text":["\r 80%|████████  | 4/5 [06:25<01:36, 96.42s/it]"]},{"name":"stdout","output_type":"stream","text":["Epoch 5: Train loss=0.7250514135230333, Eval score=0.5\n","current template: {\"placeholder\": \"text_a\"} It was {\"mask\"} . It was horrible.the plot is nothing but boilerplate clichés from start to finish , and the script assumes that not only would subtlety be lost on the target audience , but that it 's also too stupid to realize that they 've already seen this exact same movie a hundred times It was terrible. the film presents visceral and dangerously honest revelations about the men and machines behind the curtains of our planet . It was great., wrapped example: [[{'text': 'nothing happens , and it happens to flat characters .', 'loss_ids': 0, 'shortenable_ids': 1}, {'text': ' It was', 'loss_ids': 0, 'shortenable_ids': 0}, {'text': '<mask>', 'loss_ids': 1, 'shortenable_ids': 0}, {'text': \" . It was horrible.the plot is nothing but boilerplate clichés from start to finish , and the script assumes that not only would subtlety be lost on the target audience , but that it 's also too stupid to realize that they 've already seen this exact same movie a hundred times It was terrible. the film presents visceral and dangerously honest revelations about the men and machines behind the curtains of our planet . It was great.\", 'loss_ids': 0, 'shortenable_ids': 0}], {'guid': 'train-0', 'label': 0}]\n"]},{"name":"stderr","output_type":"stream","text":["\n","tokenizing: 32it [00:00, 714.48it/s]\n","\n","tokenizing: 32it [00:00, 736.06it/s]\n","/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 1: Train loss=1.8709437839133898, Eval score=0.5\n","Epoch 2: Train loss=0.7510067372932099, Eval score=0.5\n","Epoch 3: Train loss=0.7953800254035741, Eval score=0.5\n","Epoch 4: Train loss=0.7837908999063075, Eval score=0.5\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 5/5 [08:02<00:00, 96.42s/it]"]},{"name":"stdout","output_type":"stream","text":["Epoch 5: Train loss=0.9271215732442215, Eval score=0.5\n","final best template: {\"placeholder\": \"text_a\"} It was {\"mask\"} ..the plot is nothing but boilerplate clichés from start to finish , and the script assumes that not only would subtlety be lost on the target audience , but that it 's also too stupid to realize that they 've already seen this exact same movie a hundred times It was terrible. the film presents visceral and dangerously honest revelations about the men and machines behind the curtains of our planet . It was great.\n","wrapped example: [[{'text': 'nothing happens , and it happens to flat characters .', 'loss_ids': 0, 'shortenable_ids': 1}, {'text': ' It was', 'loss_ids': 0, 'shortenable_ids': 0}, {'text': '<mask>', 'loss_ids': 1, 'shortenable_ids': 0}, {'text': \" . It was horrible.the plot is nothing but boilerplate clichés from start to finish , and the script assumes that not only would subtlety be lost on the target audience , but that it 's also too stupid to realize that they 've already seen this exact same movie a hundred times It was terrible. the film presents visceral and dangerously honest revelations about the men and machines behind the curtains of our planet . It was great.\", 'loss_ids': 0, 'shortenable_ids': 0}], {'guid': 'train-0', 'label': 0}]\n"]},{"name":"stderr","output_type":"stream","text":["\n"]}],"source":["from tqdm import tqdm\n","\n","class ManualTemplateWithoutParse(ManualTemplate):\n","    \"\"\"The generated template from TemplateGenerator is a list of dict of parsed template_text. So no further parsing is needed.\"\"\"\n","    def on_text_set(self):\n","        pass\n","\n","# Template generation\n","if auto_t:\n","    print('performing auto_t...')\n","\n","    if cuda:\n","        template_generate_model = template_generate_model.cuda()\n","\n","    # Creates an instance of T5TemplateGenerator, used for generating text templates\n","    template_generator = T5TemplateGenerator(template_generate_model, template_generate_tokenizer, template_tokenizer_wrapper, verbalizer, beam_width=5) # Beam_width is set to 5 here for efficiency; to improve performance, try a larger number.\n","\n","\n","    dataloader = PromptDataLoader(dataset['train'], template, tokenizer=template_generate_tokenizer, tokenizer_wrapper_class=template_tokenizer_wrapper, batch_size=len(dataset['train']), decoder_max_length=128, max_seq_length=128, shuffle=False, teacher_forcing=False) # Register all data at once\n","    for data in dataloader:\n","        if cuda:\n","            data = data.cuda()\n","        template_generator._register_buffer(data)\n","\n","    template_generate_model.eval()\n","    print('generating...')\n","    template_texts = template_generator._get_templates() # Calls _get_templates on template_generator to generate template texts.\n","\n","    # Converting and Printing Templates\n","    original_template = template.text\n","    template_texts = [template_generator.convert_template(template_text, original_template) for template_text in template_texts]\n","    # template_generator._show_template()\n","    template_generator.release_memory()\n","    # Generate a number of candidate template text\n","    print(template_texts)\n","\n","    # Iterate over each candidate and select the best one\n","    best_metrics = 0.0\n","    best_template_text = None\n","    for template_text in tqdm(template_texts):\n","        verbalizer = ManualVerbalizer(tokenizer=tokenizer, num_classes=2, label_words=[['terrible'],['great']])\n","        template = LMBFFTemplateGenerationTemplate(tokenizer=template_generate_tokenizer, verbalizer=verbalizer, text=template_text)\n","        print(f\"current template: {template_text}, wrapped example: {template.wrap_one_example(dataset['train'][0])}\")\n","\n","        train_dataloader = PromptDataLoader(dataset['train'], template, tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass, shuffle=True)\n","        valid_dataloader = PromptDataLoader(dataset['validation'], template, tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass)\n","\n","        model = PromptForClassification(copy.deepcopy(plm), template, verbalizer)\n","\n","        loss_func = torch.nn.CrossEntropyLoss()\n","        no_decay = ['bias', 'LayerNorm.weight']\n","        # it's always good practice to set no decay to bias and LayerNorm parameters\n","        optimizer_grouped_parameters = [\n","            {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n","            {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n","        ]\n","\n","        optimizer = AdamW(optimizer_grouped_parameters, lr=2e-5)\n","        if cuda:\n","            model = model.cuda()\n","        score = fit(model, train_dataloader, valid_dataloader, loss_func, optimizer)\n","\n","        #######################################################\n","        # TODO: Use score to Find your best template_text     #\n","        #######################################################\n","        if score > best_metrics:\n","            best_metrics = score\n","            best_template_text = template_text\n","        #######################################################\n","        #                 End of your code                    #\n","        #######################################################\n","    # Use the best template\n","    verbalizer = ManualVerbalizer(tokenizer=tokenizer, num_classes=2, label_words=[['terrible'],['great']])\n","    template = LMBFFTemplateGenerationTemplate(tokenizer=template_generate_tokenizer, verbalizer=verbalizer, text=template_text)\n","    print(\"final best template:\", best_template_text)\n","    print(\"wrapped example:\", template.wrap_one_example(dataset[\"train\"][0]))"]},{"cell_type":"markdown","metadata":{"id":"AsqQhb_4R51K"},"source":["# Automatic erbalizer generation"]},{"cell_type":"markdown","metadata":{"id":"N7sctiL5R51K"},"source":["Verbalizer template from VerbalizerGenerator and find the best verbalizer"]},{"cell_type":"code","execution_count":22,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1934225,"status":"ok","timestamp":1703087416616,"user":{"displayName":"m6","userId":"16445725425625979130"},"user_tz":-480},"id":"0_HfxfNTR51K","outputId":"fb8ed76c-f0c5-48fd-8b60-ee4c871d4132"},"outputs":[{"name":"stdout","output_type":"stream","text":["performing auto_v...\n"]},{"name":"stderr","output_type":"stream","text":["tokenizing: 32it [00:00, 725.68it/s]\n","  0%|          | 0/20 [00:00<?, ?it/s]\n","tokenizing: 32it [00:00, 427.00it/s]\n","\n","tokenizing: 32it [00:00, 410.43it/s]\n","/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 1: Train loss=1.7657028905023253, Eval score=0.59375\n","Epoch 2: Train loss=1.1723438473254646, Eval score=0.5\n","Epoch 3: Train loss=0.6553019889979623, Eval score=0.625\n","Epoch 4: Train loss=0.052497891083476134, Eval score=0.8125\n"]},{"name":"stderr","output_type":"stream","text":["\r  5%|▌         | 1/20 [01:36<30:31, 96.41s/it]"]},{"name":"stdout","output_type":"stream","text":["Epoch 5: Train loss=0.08518365165082287, Eval score=0.59375\n"]},{"name":"stderr","output_type":"stream","text":["\n","tokenizing: 32it [00:00, 690.72it/s]\n","\n","tokenizing: 32it [00:00, 696.10it/s]\n","/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 1: Train loss=1.2251756465811923, Eval score=0.875\n","Epoch 2: Train loss=0.618511427633166, Eval score=0.8125\n","Epoch 3: Train loss=0.3142410656910215, Eval score=0.875\n","Epoch 4: Train loss=0.04954465843684375, Eval score=0.78125\n"]},{"name":"stderr","output_type":"stream","text":["\r 10%|█         | 2/20 [03:12<28:54, 96.38s/it]"]},{"name":"stdout","output_type":"stream","text":["Epoch 5: Train loss=0.2673700284250913, Eval score=0.8125\n"]},{"name":"stderr","output_type":"stream","text":["\n","tokenizing: 32it [00:00, 737.69it/s]\n","\n","tokenizing: 32it [00:00, 730.68it/s]\n","/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 1: Train loss=1.6883074304205365, Eval score=0.5\n","Epoch 2: Train loss=0.634735889849253, Eval score=0.9375\n","Epoch 3: Train loss=0.03514791459929256, Eval score=0.75\n","Epoch 4: Train loss=0.001213982088756893, Eval score=0.75\n"]},{"name":"stderr","output_type":"stream","text":["\r 15%|█▌        | 3/20 [04:49<27:19, 96.44s/it]"]},{"name":"stdout","output_type":"stream","text":["Epoch 5: Train loss=0.00041373303918135207, Eval score=0.75\n"]},{"name":"stderr","output_type":"stream","text":["\n","tokenizing: 32it [00:00, 700.41it/s]\n","\n","tokenizing: 32it [00:00, 700.96it/s]\n","/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 1: Train loss=1.2603721116553288, Eval score=0.5\n","Epoch 2: Train loss=0.7579924503806978, Eval score=0.5\n","Epoch 3: Train loss=0.7307041819731239, Eval score=0.5\n","Epoch 4: Train loss=0.7375046951410695, Eval score=0.84375\n"]},{"name":"stderr","output_type":"stream","text":["\r 20%|██        | 4/20 [06:25<25:42, 96.42s/it]"]},{"name":"stdout","output_type":"stream","text":["Epoch 5: Train loss=0.4866563026662334, Eval score=0.75\n"]},{"name":"stderr","output_type":"stream","text":["\n","tokenizing: 32it [00:00, 677.56it/s]\n","\n","tokenizing: 32it [00:00, 685.88it/s]\n","/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 1: Train loss=1.916273319460089, Eval score=0.96875\n","Epoch 2: Train loss=0.05207285639471593, Eval score=0.875\n","Epoch 3: Train loss=0.6852029143516916, Eval score=0.8125\n","Epoch 4: Train loss=0.19858161852255307, Eval score=0.75\n"]},{"name":"stderr","output_type":"stream","text":["\r 25%|██▌       | 5/20 [08:02<24:06, 96.40s/it]"]},{"name":"stdout","output_type":"stream","text":["Epoch 5: Train loss=0.011336769446316453, Eval score=0.8125\n"]},{"name":"stderr","output_type":"stream","text":["\n","tokenizing: 32it [00:00, 684.15it/s]\n","\n","tokenizing: 32it [00:00, 713.98it/s]\n","/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 1: Train loss=1.8550432200718205, Eval score=0.5\n","Epoch 2: Train loss=0.8179998595733196, Eval score=0.625\n","Epoch 3: Train loss=0.29885483684483916, Eval score=0.78125\n","Epoch 4: Train loss=0.012552359361706067, Eval score=0.84375\n"]},{"name":"stderr","output_type":"stream","text":["\r 30%|███       | 6/20 [09:38<22:28, 96.35s/it]"]},{"name":"stdout","output_type":"stream","text":["Epoch 5: Train loss=0.0006708913147761564, Eval score=0.84375\n"]},{"name":"stderr","output_type":"stream","text":["\n","tokenizing: 32it [00:00, 543.64it/s]\n","\n","tokenizing: 32it [00:00, 697.95it/s]\n","/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 1: Train loss=1.1865657639368692, Eval score=0.875\n","Epoch 2: Train loss=0.16339186250991133, Eval score=0.8125\n","Epoch 3: Train loss=0.9249195964838464, Eval score=0.5\n","Epoch 4: Train loss=0.7828462684410624, Eval score=0.5\n"]},{"name":"stderr","output_type":"stream","text":["\r 35%|███▌      | 7/20 [11:14<20:53, 96.41s/it]"]},{"name":"stdout","output_type":"stream","text":["Epoch 5: Train loss=0.398171778597316, Eval score=0.6875\n"]},{"name":"stderr","output_type":"stream","text":["\n","tokenizing: 32it [00:00, 688.26it/s]\n","\n","tokenizing: 32it [00:00, 679.06it/s]\n","/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 1: Train loss=1.3561965871194006, Eval score=0.59375\n","Epoch 2: Train loss=0.5846073999200598, Eval score=0.78125\n","Epoch 3: Train loss=0.11977347274660133, Eval score=0.84375\n","Epoch 4: Train loss=0.19988532746674537, Eval score=0.625\n"]},{"name":"stderr","output_type":"stream","text":["\r 40%|████      | 8/20 [12:51<19:16, 96.39s/it]"]},{"name":"stdout","output_type":"stream","text":["Epoch 5: Train loss=0.03484315163059648, Eval score=0.78125\n"]},{"name":"stderr","output_type":"stream","text":["\n","tokenizing: 32it [00:00, 629.64it/s]\n","\n","tokenizing: 32it [00:00, 584.20it/s]\n","/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 1: Train loss=2.375921554645167, Eval score=0.625\n","Epoch 2: Train loss=0.41194066968637344, Eval score=0.8125\n","Epoch 3: Train loss=0.19754561316813124, Eval score=0.84375\n","Epoch 4: Train loss=0.03516634896004689, Eval score=0.78125\n"]},{"name":"stderr","output_type":"stream","text":["\r 45%|████▌     | 9/20 [14:27<17:39, 96.35s/it]"]},{"name":"stdout","output_type":"stream","text":["Epoch 5: Train loss=0.29287354338680416, Eval score=0.78125\n"]},{"name":"stderr","output_type":"stream","text":["\n","tokenizing: 32it [00:00, 594.03it/s]\n","\n","tokenizing: 32it [00:00, 711.58it/s]\n","/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 1: Train loss=2.9600894812247125, Eval score=0.65625\n","Epoch 2: Train loss=1.2179105198010802, Eval score=0.5\n","Epoch 3: Train loss=0.7708795150974765, Eval score=0.5\n","Epoch 4: Train loss=0.7753175176985678, Eval score=0.625\n"]},{"name":"stderr","output_type":"stream","text":["\r 50%|█████     | 10/20 [16:03<16:03, 96.38s/it]"]},{"name":"stdout","output_type":"stream","text":["Epoch 5: Train loss=1.0151553229661658, Eval score=0.5\n"]},{"name":"stderr","output_type":"stream","text":["\n","tokenizing: 32it [00:00, 358.59it/s]\n","\n","tokenizing: 32it [00:00, 436.50it/s]\n","/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 1: Train loss=2.6908590438723152, Eval score=0.5\n","Epoch 2: Train loss=0.9789875950664282, Eval score=0.5\n","Epoch 3: Train loss=0.8338060115929693, Eval score=0.5\n","Epoch 4: Train loss=1.0909411490429193, Eval score=0.5\n"]},{"name":"stderr","output_type":"stream","text":["\r 55%|█████▌    | 11/20 [17:40<14:27, 96.37s/it]"]},{"name":"stdout","output_type":"stream","text":["Epoch 5: Train loss=1.0764908977853338, Eval score=0.5\n"]},{"name":"stderr","output_type":"stream","text":["\n","tokenizing: 32it [00:00, 408.13it/s]\n","\n","tokenizing: 32it [00:00, 456.32it/s]\n","/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 1: Train loss=2.1075424722112075, Eval score=0.65625\n","Epoch 2: Train loss=0.8068655519746244, Eval score=0.84375\n","Epoch 3: Train loss=0.5206548544229008, Eval score=0.78125\n","Epoch 4: Train loss=0.06017939602406841, Eval score=0.875\n"]},{"name":"stderr","output_type":"stream","text":["\r 60%|██████    | 12/20 [19:17<12:52, 96.53s/it]"]},{"name":"stdout","output_type":"stream","text":["Epoch 5: Train loss=0.005918838058160736, Eval score=0.75\n"]},{"name":"stderr","output_type":"stream","text":["\n","tokenizing: 32it [00:00, 395.55it/s]\n","\n","tokenizing: 32it [00:00, 414.09it/s]\n","/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 1: Train loss=1.6790281946850882, Eval score=0.5\n","Epoch 2: Train loss=0.30964023718843237, Eval score=0.90625\n","Epoch 3: Train loss=0.21500481176735775, Eval score=0.84375\n","Epoch 4: Train loss=0.3423951371387375, Eval score=0.875\n"]},{"name":"stderr","output_type":"stream","text":["\r 65%|██████▌   | 13/20 [20:53<11:15, 96.57s/it]"]},{"name":"stdout","output_type":"stream","text":["Epoch 5: Train loss=0.366191824249654, Eval score=0.875\n"]},{"name":"stderr","output_type":"stream","text":["\n","tokenizing: 32it [00:00, 408.81it/s]\n","\n","tokenizing: 32it [00:00, 414.45it/s]\n","/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 1: Train loss=1.4002459470648319, Eval score=0.78125\n","Epoch 2: Train loss=0.927568392129615, Eval score=0.5\n","Epoch 3: Train loss=0.7437575282820035, Eval score=0.8125\n","Epoch 4: Train loss=0.3560729284054105, Eval score=0.75\n"]},{"name":"stderr","output_type":"stream","text":["\r 70%|███████   | 14/20 [22:30<09:39, 96.55s/it]"]},{"name":"stdout","output_type":"stream","text":["Epoch 5: Train loss=0.06862242189890821, Eval score=0.875\n"]},{"name":"stderr","output_type":"stream","text":["\n","tokenizing: 32it [00:00, 639.52it/s]\n","\n","tokenizing: 32it [00:00, 706.57it/s]\n","/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 1: Train loss=2.315241795469735, Eval score=0.5\n","Epoch 2: Train loss=0.9148607125971466, Eval score=0.53125\n","Epoch 3: Train loss=0.9285631866659969, Eval score=0.5\n","Epoch 4: Train loss=0.47590798355508923, Eval score=0.84375\n"]},{"name":"stderr","output_type":"stream","text":["\r 75%|███████▌  | 15/20 [24:06<08:02, 96.51s/it]"]},{"name":"stdout","output_type":"stream","text":["Epoch 5: Train loss=0.0018508829449075392, Eval score=0.78125\n"]},{"name":"stderr","output_type":"stream","text":["\n","tokenizing: 32it [00:00, 711.87it/s]\n","\n","tokenizing: 32it [00:00, 700.50it/s]\n","/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 1: Train loss=2.2723732464024238, Eval score=0.5\n","Epoch 2: Train loss=0.8190343565365765, Eval score=0.5\n","Epoch 3: Train loss=1.064003179082647, Eval score=0.5\n","Epoch 4: Train loss=1.1638753187726252, Eval score=0.375\n"]},{"name":"stderr","output_type":"stream","text":["\r 80%|████████  | 16/20 [25:42<06:25, 96.44s/it]"]},{"name":"stdout","output_type":"stream","text":["Epoch 5: Train loss=1.0917830398539081, Eval score=0.5\n"]},{"name":"stderr","output_type":"stream","text":["\n","tokenizing: 32it [00:00, 449.02it/s]\n","\n","tokenizing: 32it [00:00, 459.93it/s]\n","/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 1: Train loss=1.0600089186937112, Eval score=0.90625\n","Epoch 2: Train loss=0.3549715213039235, Eval score=0.875\n","Epoch 3: Train loss=0.05758350600444828, Eval score=0.78125\n","Epoch 4: Train loss=0.0010363833709732262, Eval score=0.8125\n"]},{"name":"stderr","output_type":"stream","text":["\r 85%|████████▌ | 17/20 [27:19<04:49, 96.48s/it]"]},{"name":"stdout","output_type":"stream","text":["Epoch 5: Train loss=0.0005692872143185923, Eval score=0.84375\n"]},{"name":"stderr","output_type":"stream","text":["\n","tokenizing: 32it [00:00, 697.30it/s]\n","\n","tokenizing: 32it [00:00, 707.24it/s]\n","/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 1: Train loss=1.9478864960256033, Eval score=0.5\n","Epoch 2: Train loss=0.573514348214303, Eval score=0.8125\n","Epoch 3: Train loss=0.4705940255516907, Eval score=0.6875\n","Epoch 4: Train loss=0.6613950806495268, Eval score=0.71875\n"]},{"name":"stderr","output_type":"stream","text":["\r 90%|█████████ | 18/20 [28:55<03:12, 96.45s/it]"]},{"name":"stdout","output_type":"stream","text":["Epoch 5: Train loss=0.37104447582169087, Eval score=0.625\n"]},{"name":"stderr","output_type":"stream","text":["\n","tokenizing: 32it [00:00, 665.27it/s]\n","\n","tokenizing: 32it [00:00, 719.40it/s]\n","/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 1: Train loss=1.6518436112001211, Eval score=0.5\n","Epoch 2: Train loss=1.108332785544917, Eval score=0.78125\n","Epoch 3: Train loss=0.4571076849506408, Eval score=0.78125\n","Epoch 4: Train loss=0.14072751814182993, Eval score=0.625\n"]},{"name":"stderr","output_type":"stream","text":["\r 95%|█████████▌| 19/20 [30:32<01:36, 96.49s/it]"]},{"name":"stdout","output_type":"stream","text":["Epoch 5: Train loss=0.030644794957652044, Eval score=0.625\n"]},{"name":"stderr","output_type":"stream","text":["\n","tokenizing: 32it [00:00, 554.76it/s]\n","\n","tokenizing: 32it [00:00, 645.90it/s]\n","/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 1: Train loss=1.9123536473020977, Eval score=0.6875\n","Epoch 2: Train loss=0.42497683844703715, Eval score=0.84375\n","Epoch 3: Train loss=0.5367676667483465, Eval score=0.9375\n","Epoch 4: Train loss=0.031749989980525584, Eval score=0.90625\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 20/20 [32:08<00:00, 96.45s/it]"]},{"name":"stdout","output_type":"stream","text":["Epoch 5: Train loss=0.0009139228913068731, Eval score=0.90625\n","final best label words: ['boring', 'fantastic']\n"]},{"name":"stderr","output_type":"stream","text":["\n"]}],"source":["# Verbalizer generation\n","from openprompt.prompts.prompt_generator import RobertaVerbalizerGenerator\n","if auto_v:\n","    print('performing auto_v...')\n","    # Load generation model for verbalizer generation\n","    if cuda:\n","        plm = plm.cuda()\n","\n","    # Creates an instance of RobertaVerbalizerGenerator, used for generating verbalizer.\n","    verbalizer_generator = RobertaVerbalizerGenerator(model=plm, tokenizer=tokenizer, candidate_num=20, label_word_num_per_class=20) # To improve performance, try larger numbers\n","\n","\n","    dataloader = PromptDataLoader(dataset['train'], template, tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass, batch_size=32)\n","    for data in dataloader:\n","        if cuda:\n","            data = data.cuda()\n","        verbalizer_generator.register_buffer(data)\n","\n","    # Calls generate on verbalizer_generator to generate label words.\n","    label_words_list = verbalizer_generator.generate()\n","    verbalizer_generator.release_memory()\n","\n","    # Iterate over each candidate and select the best one\n","    current_verbalizer = copy.deepcopy(verbalizer)\n","    best_metrics = 0.0\n","    best_label_words = None\n","    for label_words in tqdm(label_words_list):\n","        current_verbalizer.label_words = label_words\n","        train_dataloader = PromptDataLoader(dataset['train'], template, tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass, shuffle=True)\n","        valid_dataloader = PromptDataLoader(dataset['validation'], template, tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass)\n","\n","        model = PromptForClassification(copy.deepcopy(plm), template, current_verbalizer)\n","\n","        loss_func = torch.nn.CrossEntropyLoss()\n","        no_decay = ['bias', 'LayerNorm.weight']\n","        # it's always good practice to set no decay to bias and LayerNorm parameters\n","        optimizer_grouped_parameters = [\n","            {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n","            {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n","        ]\n","\n","        optimizer = AdamW(optimizer_grouped_parameters, lr=2e-5)\n","        if cuda:\n","            model = model.cuda()\n","        score = fit(model, train_dataloader, valid_dataloader, loss_func, optimizer)\n","\n","        #######################################################\n","        # TODO: Use score to find your best_label_word        #\n","        #######################################################\n","        if score > best_metrics:\n","            best_metrics = score\n","            best_label_words = label_words\n","        #######################################################\n","        #                 End of your code                    #\n","        #######################################################\n","    # use the best verbalizer\n","    print(\"final best label words:\", best_label_words)\n","    verbalizer = ManualVerbalizer(tokenizer, num_classes=2, label_words=best_label_words)"]},{"cell_type":"markdown","metadata":{"id":"PDHB6yOQR51K"},"source":["# Evaluation"]},{"cell_type":"code","execution_count":23,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":98780,"status":"ok","timestamp":1703087515370,"user":{"displayName":"m6","userId":"16445725425625979130"},"user_tz":-480},"id":"g8iaALa4R51K","outputId":"bad741ee-4705-4e85-b287-06dc1b42465c"},"outputs":[{"name":"stderr","output_type":"stream","text":["tokenizing: 32it [00:00, 453.13it/s]\n","tokenizing: 32it [00:00, 451.01it/s]\n","tokenizing: 872it [00:02, 369.88it/s]\n","/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 1: Train loss=1.9016268969280645, Eval score=0.5\n","Epoch 2: Train loss=0.9475884341809433, Eval score=0.5\n","Epoch 3: Train loss=0.15161352850878984, Eval score=0.875\n","Epoch 4: Train loss=0.4173467281165415, Eval score=0.8125\n","Epoch 5: Train loss=0.04197401693818392, Eval score=0.875\n"]}],"source":["train_dataloader = PromptDataLoader(dataset['train'], template, tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass, shuffle=True)\n","valid_dataloader = PromptDataLoader(dataset['validation'], template, tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass)\n","test_dataloader = PromptDataLoader(dataset['test'], template, tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass)\n","\n","\n","model = PromptForClassification(copy.deepcopy(plm), template, verbalizer)\n","loss_func = torch.nn.CrossEntropyLoss()\n","no_decay = ['bias', 'LayerNorm.weight']\n","# It's always good practice to set no decay to bias and LayerNorm parameters\n","optimizer_grouped_parameters = [\n","    {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n","    {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n","]\n","\n","optimizer = AdamW(optimizer_grouped_parameters, lr=2e-5)\n","if cuda:\n","    model = model.cuda()\n","score = fit(model, train_dataloader, valid_dataloader, loss_func, optimizer)"]},{"cell_type":"code","execution_count":24,"metadata":{"executionInfo":{"elapsed":38718,"status":"ok","timestamp":1703087611866,"user":{"displayName":"m6","userId":"16445725425625979130"},"user_tz":-480},"id":"23aLyQGSvoxN"},"outputs":[],"source":["torch.save(model.state_dict(), 'ckpts/task3_temp2_875.pt')"]},{"cell_type":"markdown","metadata":{"id":"S9nKbDH7R51L"},"source":["# Prediction\n","\n","Predict the results based on testing set. Upload to [Kaggle](https://www.kaggle.com/t/5b8876ed26fd495b8353ad7ce94b6f65)."]},{"cell_type":"code","execution_count":17,"metadata":{"executionInfo":{"elapsed":107965,"status":"ok","timestamp":1703082141823,"user":{"displayName":"m6","userId":"16445725425625979130"},"user_tz":-480},"id":"yvXJV6--R51L"},"outputs":[],"source":["model.eval()\n","\n","allpreds = []\n","for step, inputs in enumerate(test_dataloader):\n","    if cuda:\n","        inputs = inputs.cuda()\n","    logits = model(inputs)\n","    allpreds.extend(torch.argmax(logits, dim=-1).cpu().tolist())\n","\n","with open('ckpts/pred.csv', 'w') as f:\n","    f.write('index,sentiment_label\\n')\n","    for i, pred in enumerate(allpreds):\n","        f.write('{},{}\\n'.format(i, pred))"]},{"cell_type":"markdown","metadata":{"id":"M48pNrCqR51L"},"source":["# Report (15 points)\n","\n","- Task 1: Compare **two** different models you employed and provide a brief discussion of your implementation.\n","\n","- Task 2: You need to try at least **three** different templates and verbalizers to compare how your prompts work with the model. Report your performance in zero-shot, one-shot, and few-shot scenarios, with examples drawn from the training set.\n","\n","- Task 3: Try at least three different manually crafted templates to compare them with auto-generated templates. Evaluate the performance with different numbers of demonstrations and plot the graph from Figure 3 in the paper (https://arxiv.org/pdf/2012.15723.pdf). Also, report your best template and verbalizer.\n"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3.10.9","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.9"},"vscode":{"interpreter":{"hash":"51ee1b965d6f75a20b2b6babb72920dce4fab5775c12eb1659af0fb55d185fed"}}},"nbformat":4,"nbformat_minor":0}
